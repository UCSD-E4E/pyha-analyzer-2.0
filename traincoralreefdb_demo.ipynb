{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c4c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef28f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyha_analyzer import PyhaTrainer, PyhaTrainingArguments, extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccffaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600c9c6dc33248ccb0a24462603b540c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df1e27dab9a49899f39f26cf39e6732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1921a64b564370a660687fb06bbe07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sample_rate', 'labels', 'filepath', 'audio', 'audio_in'],\n",
       "        num_rows: 703\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['sample_rate', 'labels', 'filepath', 'audio', 'audio_in'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sample_rate', 'labels', 'filepath', 'audio', 'audio_in'],\n",
       "        num_rows: 212\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coralreef_extractor = extractors.CoralReef()\n",
    "coral_ads = coralreef_extractor(\"/home/s.kamboj.400/unzipped-coral\")\n",
    "coral_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978c9b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['Degraded_Reef', 'Non_Degraded_Reef'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coral_ads[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39826636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_rate': 48000,\n",
       " 'labels': [1, 0],\n",
       " 'filepath': '/home/s.kamboj.400/unzipped-coral/Degraded_Reef/Pavonas_June2025/20250128_065717.WAV',\n",
       " 'audio': {'path': '/home/s.kamboj.400/unzipped-coral/Degraded_Reef/Pavonas_June2025/20250128_065717.WAV',\n",
       "  'array': array([ 0.03729248,  0.01147461,  0.06411743, ...,  0.05410767,\n",
       "          0.02902222, -0.03527832], shape=(1824000,)),\n",
       "  'sampling_rate': 48000},\n",
       " 'audio_in': {'array': '/home/s.kamboj.400/unzipped-coral/Degraded_Reef/Pavonas_June2025/20250128_065717.WAV',\n",
       "  'sampling_rate': 48000}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coral_ads[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c0c09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.10119695)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyha_analyzer.preprocessors import MelSpectrogramPreprocessors\n",
    "\n",
    "# TODO: allow for normalization system\n",
    "\n",
    "# preprocessor acts as a function for processing\n",
    "# class allows us to configure parameters and whatnot\n",
    "preprocessor = MelSpectrogramPreprocessors(duration=5, class_list=coral_ads[\"train\"].features[\"labels\"].feature.names)\n",
    "\n",
    "coral_ads[\"train\"].set_transform(preprocessor)\n",
    "coral_ads[\"valid\"].set_transform(preprocessor)\n",
    "coral_ads[\"test\"].set_transform(preprocessor)\n",
    "coral_ads[\"train\"][[0, 1]][\"audio\"][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad7415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_rate': 48000,\n",
       " 'labels': array([0., 1.], dtype=float32),\n",
       " 'filepath': '/home/s.kamboj.400/unzipped-coral/Non_Degraded_Reef/July_2024/20240716_091000.WAV',\n",
       " 'audio': array([[[0.        , 0.        , 0.        , ..., 0.00784314,\n",
       "          0.00784314, 0.00392157],\n",
       "         [0.        , 0.        , 0.        , ..., 0.01960784,\n",
       "          0.01568628, 0.01176471],\n",
       "         [0.        , 0.        , 0.        , ..., 0.00784314,\n",
       "          0.00392157, 0.00784314],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]], shape=(1, 256, 431), dtype=float32),\n",
       " 'audio_in': array([[[0.        , 0.        , 0.        , ..., 0.00784314,\n",
       "          0.00784314, 0.00392157],\n",
       "         [0.        , 0.        , 0.        , ..., 0.01960784,\n",
       "          0.01568628, 0.01176471],\n",
       "         [0.        , 0.        , 0.        , ..., 0.00784314,\n",
       "          0.00392157, 0.00784314],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]], shape=(1, 256, 431), dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coral_ads[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f83c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyha_analyzer.models import EfficentNet\n",
    "#model = EfficentNet(num_classes=len(coral_ads[\"train\"].features[\"ebird_code\"].names))\n",
    "model = EfficentNet(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da0fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msskamboj\u001b[0m (\u001b[33macoustic-species-identification\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelAveragePrecision init with num_labels  2  and average  none\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/s.kamboj.400/pyha-analyzer-2.0/wandb/run-20250711_101310-8o17zrup</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/acoustic-species-identification/pa2.0_test/runs/8o17zrup' target=\"_blank\">working_dir</a></strong> to <a href='https://wandb.ai/acoustic-species-identification/pa2.0_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/acoustic-species-identification/pa2.0_test' target=\"_blank\">https://wandb.ai/acoustic-species-identification/pa2.0_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/acoustic-species-identification/pa2.0_test/runs/8o17zrup' target=\"_blank\">https://wandb.ai/acoustic-species-identification/pa2.0_test/runs/8o17zrup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11, training_loss=0.5831011533737183, metrics={'train_runtime': 69.9864, 'train_samples_per_second': 10.045, 'train_steps_per_second': 0.157, 'total_flos': 0.0, 'train_loss': 0.5831011533737183, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyha_analyzer import constants\n",
    "args = PyhaTrainingArguments(\n",
    "    working_dir=\"working_dir\",\n",
    "    run_name= constants.DEFAULT_RUN_NAME,\n",
    "    project_name=constants.DEFAULT_PROJECT_NAME\n",
    ")\n",
    "args.num_train_epochs = 1\n",
    "args.eval_steps = 20\n",
    "\n",
    "\n",
    "trainer = PyhaTrainer(\n",
    "    model=model,\n",
    "    dataset=coral_ads,\n",
    "    training_args=args,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3606eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to save the git commit hash and model state_dict as pt file\n",
    "# import subprocess\n",
    "# import torch\n",
    "\n",
    "# git_hash = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n",
    "\n",
    "# with open(\"training_info.txt\", \"w\") as f:\n",
    "#     f.write(f\"Git commit: {git_hash}\\n\")\n",
    "\n",
    "# #save model in .pt file associated with git hash. This is useful for reproducibility, so that we can always refer back to the exact code used for training.\n",
    "# model_save_path = f\"coral_model_{git_hash[:7]}.pt\"\n",
    "# torch.save(trainer.model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3e98f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Soundscape_loss': 0.6803421378135681,\n",
       " 'Soundscape_cMAP': 0.4800085425376892,\n",
       " 'Soundscape_ROCAUC': 0.4487537145614624,\n",
       " 'Soundscape_runtime': 15.4209,\n",
       " 'Soundscape_samples_per_second': 13.748,\n",
       " 'Soundscape_steps_per_second': 0.454,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=coral_ads[\"test\"], metric_key_prefix=\"Soundscape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5fa1da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelAveragePrecision init with num_labels  2  and average  none\n",
      "Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s.kamboj.400/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    }
   ],
   "source": [
    "from pyha_analyzer.metrics.gradcam import GradCAM\n",
    "import torch\n",
    "from pyha_analyzer.models import EfficentNet\n",
    "from pyha_analyzer import constants, PyhaTrainer, PyhaTrainingArguments\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "\n",
    "# Rebuild and load model, need to do this rather than train on fake model; otherwise, outputs are wrong\n",
    "newModel = EfficentNet(num_classes=2)\n",
    "newModel.load_state_dict(torch.load(\"coral_model_4146a1f.pt\"))\n",
    "newModel.eval()\n",
    "\n",
    "# Rebuild trainer\n",
    "args = PyhaTrainingArguments(\n",
    "    working_dir=\"working_dir\",\n",
    "    run_name=constants.DEFAULT_RUN_NAME,\n",
    "    project_name=constants.DEFAULT_PROJECT_NAME\n",
    ")\n",
    "args.num_train_epochs = 1\n",
    "args.eval_steps = 20\n",
    "\n",
    "trainer = PyhaTrainer(\n",
    "    model=newModel,\n",
    "    dataset=coral_ads,\n",
    "    training_args=args,\n",
    ")\n",
    "\n",
    "\n",
    "target_layer = newModel.model.efficientnet.encoder.top_conv\n",
    "print(target_layer)\n",
    "gradcam = GradCAM(newModel.model, target_layer) # lets you use huggingface model code not the Pyha wrapper because you want to use that forward command\n",
    "\n",
    "\n",
    "pdfPathDegraded = \"Degraded_gradCAM.pdf\"\n",
    "pdfPathNonDegraded = \"Non_Degraded_gradCAM.pdf\" \n",
    "pDegraded = PdfPages(pdfPathDegraded) \n",
    "pNonDegraded = PdfPages(pdfPathNonDegraded)\n",
    "countDegraded=0\n",
    "countNonDegraded=0\n",
    "numGradCams=129 # make it 99 to get 100 degraded & 100 non degraded\n",
    "all_cam_pixels_degraded = []\n",
    "all_cam_pixels_nondegraded = []\n",
    "\n",
    "#cannot batch for grad cam\n",
    "for i, item in enumerate(coral_ads[\"train\"]):\n",
    "    input_tensor = torch.tensor(item[\"audio_in\"], dtype=torch.float32).unsqueeze(0)\n",
    "    cam_output = gradcam.generate(input_tensor)\n",
    "    cam_flat = cam_output.flatten()\n",
    "\n",
    "    # generate (numGradCam +1) quantity of grad cams and save it in the pdf\n",
    "    if ((countDegraded <=numGradCams) and (item[\"labels\"] == [1,0]).all()):\n",
    "        all_cam_pixels_degraded.extend(cam_flat)\n",
    "        gradcam.show_gradcam_overlay(item, cam_output, pDegraded)\n",
    "        countDegraded+=1\n",
    "    elif ((countNonDegraded <=numGradCams) and (item[\"labels\"] ==[ 0, 1]).all()):\n",
    "        all_cam_pixels_nondegraded.extend(cam_flat)\n",
    "        gradcam.show_gradcam_overlay(item, cam_output, pNonDegraded)\n",
    "        countNonDegraded+=1\n",
    "    if ((countNonDegraded > numGradCams) & (countDegraded > numGradCams)):\n",
    "        break\n",
    "\n",
    "pDegraded.close()\n",
    "pNonDegraded.close()\n",
    "\n",
    "#convert so that you can make histogram\n",
    "degraded_vals = np.array(all_cam_pixels_degraded)\n",
    "nondegraded_vals = np.array(all_cam_pixels_nondegraded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.model) #make sure u get the lasst conv layer! check this to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec535b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "#Connect to LanceDB and save the embeddings\n",
    "uri = \"database/coral_reef_db.lance\"\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "112fbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the table for testing purposes, so that the same embeddings are not re-inserted because id is simply a key not a primary key, so embeddings can get reinserted\n",
    "if \"coral_embeddings\" in db.table_names():\n",
    "    #print(\"Table exists. If you run the next couple code blocks again, then you will get duplicate embeddings.\")\n",
    "    db.drop_table(\"coral_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc9b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "schema = pa.schema([\n",
    "    pa.field(\"id\", pa.string()),\n",
    "    pa.field(\"vector_embedding\", pa.list_(pa.float32(), list_size=1280)),\n",
    "    pa.field(\"label\", pa.string()),\n",
    "    pa.field(\"audio_path\", pa.string())\n",
    "])\n",
    "table = db.create_table(\"coral_embeddings\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9909e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddResult(version=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "batch_size = 32\n",
    "table_rows=[]\n",
    "\n",
    "for split in [\"train\", \"valid\"]:\n",
    "    dataset = coral_ads[split]\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_items = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "\n",
    "        # build input tensor\n",
    "        batch_inputs = torch.stack([\n",
    "            torch.tensor(item[\"audio_in\"], dtype=torch.float32)\n",
    "            for item in batch_items\n",
    "        ])\n",
    "\n",
    "        # get embeddings through batching\n",
    "        embeddings = model.get_embedding(batch_inputs)\n",
    "\n",
    "        # Loop through items and create metadata\n",
    "        for j, (embedding, item) in enumerate(zip(embeddings, batch_items)):\n",
    "            label_vec = item[\"labels\"]\n",
    "            if (label_vec == [0.0, 1.0]).all():\n",
    "                curr_label = \"Non_Degraded_Reef\"\n",
    "            elif (label_vec == [1.0, 0.0]).all():\n",
    "                curr_label = \"Degraded_Reef\"\n",
    "            else:\n",
    "                curr_label = \"Unknown\"\n",
    "                print(\"Unknown label:\", label_vec)\n",
    "\n",
    "            metadata = {\n",
    "                \"id\": f\"{split}{i + j}\",\n",
    "                \"vector_embedding\": embedding.tolist(),\n",
    "                \"label\": curr_label,\n",
    "                \"audio_path\": item[\"filepath\"]\n",
    "            }\n",
    "\n",
    "            table_rows.append(metadata)\n",
    "\n",
    "# Step 4: Insert all at once\n",
    "table.add(table_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b03c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_split = \"test\"\n",
    "# dataset = coral_ads[curr_split]\n",
    "# batch_size = 16  \n",
    "\n",
    "# for i in range(0, len(dataset), batch_size):\n",
    "#     # Select a batch of size 16\n",
    "#     batch_items = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "\n",
    "#     # Create batched input tensor\n",
    "#     batch_inputs = torch.stack([\n",
    "#         torch.tensor(item[\"audio_in\"], dtype=torch.float32)\n",
    "#         for item in batch_items\n",
    "#     ])  \n",
    "\n",
    "#     # 3. Get embeddings for the batch of 16\n",
    "#     embeddings = model.get_embedding(batch_inputs) \n",
    "\n",
    "#     # 4. Search LanceDB for each embedding\n",
    "#     for item, embedding in zip(batch_items, embeddings):\n",
    "#         query_vector = embedding.tolist()\n",
    "#         results = table.search(query_vector).limit(1).to_pandas()\n",
    "\n",
    "#         print(\"The most similar to\", item[\"filepath\"], \"is\", results[\"audio_path\"].tolist()[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyha-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
