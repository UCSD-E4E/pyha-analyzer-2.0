{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8149ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c153dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyha_analyzer import PyhaTrainer, extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20fab9",
   "metadata": {},
   "source": [
    "# Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eadc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorry TQ, I don't have these files mounted yet...\n",
    "# peru132_extr = extractors.Peru132Extractor()\n",
    "# peru_132_ads = peru132_extr(\"/data/XC_wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8862a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peru_132_ads = peru_132_ads.get_provenance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91dfeaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel', 'audio_in', 'labels'],\n",
       "        num_rows: 4368\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel', 'audio_in', 'labels'],\n",
       "        num_rows: 1092\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel', 'audio_in', 'labels'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birdset_extactor = extractors.Birdset()\n",
    "\n",
    "hsn_ads = birdset_extactor(\"HSN\")\n",
    "# per_ads = birdset_extactor(\"PER\")\n",
    "\n",
    "hsn_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "142339cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyha_analyzer.dataset.AudioDataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hsn_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583c0ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel', 'audio_in', 'labels'],\n",
       "    num_rows: 4368\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsn_ads[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa44f245",
   "metadata": {},
   "source": [
    "# Online Preprocessing example\n",
    "\n",
    "Suppose we just wanted spectrograms with no audio preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799ff026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bytes': None,\n",
       " 'path': '/home/s.perry.543/.cache/huggingface/datasets/downloads/extracted/e7a5318118cabfab47a509edeb627860a60537535aeea20a19fced4c110d579e/HSN_001_20150708_061805_000_005.ogg'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsn_ads[\"test\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe2c636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel', 'audio_in', 'labels'],\n",
       "        num_rows: 4368\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel', 'audio_in', 'labels'],\n",
       "        num_rows: 1092\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel', 'audio_in', 'labels'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsn_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "835d7239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.16862746, 0.53333336, 0.70980394, ..., 0.8980392 ,\n",
       "         0.40392157, 0.4117647 ],\n",
       "        [0.2784314 , 0.22745098, 0.09803922, ..., 0.2784314 ,\n",
       "         0.13725491, 0.14901961],\n",
       "        [0.03921569, 0.0627451 , 0.06666667, ..., 0.16470589,\n",
       "         0.05490196, 0.01960784],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]]], shape=(1, 128, 216), dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyha_analyzer.preprocessors import MelSpectrogramPreprocessors\n",
    "\n",
    "# preprocessor acts as a function for processing\n",
    "# class allows us to configure parameters and whatnot\n",
    "preprocessor = MelSpectrogramPreprocessors(duration=5)\n",
    "\n",
    "hsn_ads.set_transform(preprocessor)\n",
    "hsn_ads[\"train\"][[0, 1]][\"audio\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d27af",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "As a demo, implementing a model here because we haven't fleshed out the AudioDataset api yet\n",
    "\n",
    "In the future this exists in `/pyha_analyzer/models/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e83ab7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyha_analyzer.models.demo_CNN import ResnetConfig, ResnetModel\n",
    "\n",
    "# Going to make notes on anything that should be handled not here\n",
    "# This is one of these things, this should be handled by potentially pyha_trainer\n",
    "resnet50d_config = ResnetConfig(\n",
    "    num_classes=len(hsn_ads[\"train\"].features[\"ebird_code\"].names), input_channels=1\n",
    ")\n",
    "model = ResnetModel(resnet50d_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1424f590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='1638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  55/1638 01:21 < 40:25, 0.65 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m trainer = PyhaTrainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     dataset=hsn_ads,\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/accelerate/data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    575\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    576\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2781\u001b[39m, in \u001b[36mDataset.__getitems__\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m   2779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m   2780\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2781\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2782\u001b[39m     n_examples = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch.items()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:653\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    651\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:410\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:536\u001b[39m, in \u001b[36mCustomFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    534\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_arrow_extractor().extract_batch(pa_table)\n\u001b[32m    535\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/pyha_analyzer/preprocessors/spectogram_preprocessors.py:15\u001b[39m, in \u001b[36mMelSpectrogramPreprocessors.__call__\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m     13\u001b[39m new_audio = []\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m])):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     y, sr = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# padding yippeee\u001b[39;00m\n\u001b[32m     17\u001b[39m     start = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/librosa/core/audio.py:190\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Final cleanup for dtype and contiguity\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mono:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     y = \u001b[43mto_mono\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    193\u001b[39m     y = resample(y, orig_sr=sr_native, target_sr=sr, res_type=res_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/librosa/core/audio.py:505\u001b[39m, in \u001b[36mto_mono\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert an audio signal to mono by averaging samples across channels.\u001b[39;00m\n\u001b[32m    480\u001b[39m \n\u001b[32m    481\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    502\u001b[39m \u001b[33;03m(117601,)\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# Validate the buffer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalid_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    508\u001b[39m     y = np.mean(y, axis=\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(y.ndim - \u001b[32m1\u001b[39m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyha-analyzer-2.0/.venv/lib/python3.11/site-packages/librosa/util/utils.py:297\u001b[39m, in \u001b[36mvalid_audio\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[32m    294\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAudio data must be at least one-dimensional, given y.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m.all():\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\u001b[33m\"\u001b[39m\u001b[33mAudio buffer is not finite everywhere\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = PyhaTrainer(\n",
    "    model=model,\n",
    "    dataset=hsn_ads,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38eb82",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
